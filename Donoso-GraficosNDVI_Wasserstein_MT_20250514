# CODIGO DESARROLLADO POR MATIAS TREUMUN - MAYO 2025

library(sf)
library(ggplot2)
library(dplyr)
library(transport)

# Cargar capas
path_2023 <- "D:/_1341-ANGLO-RecuperacionEsteroOrtigas/CAPAS/Vegetación/2023/Donoso/20240307_2023.shp"
path_2024 <- "D:/_1341-ANGLO-RecuperacionEsteroOrtigas/CAPAS/Vegetación/2024/Donoso/20240307_2024.shp"
path_2025 <- "D:/_1341-ANGLO-RecuperacionEsteroOrtigas/CAPAS/Vegetación/Mayo_2025/Donoso/20250319_2025.shp"

# Leer shapefiles
sf_2023 <- st_read(path_2023)
sf_2024 <- st_read(path_2024)
sf_2025 <- st_read(path_2025)

# Área por píxel
area_per_pixel_2023 <- 0.000261986596
area_per_pixel_2024 <- 0.000277569761180625
area_per_pixel_2025 <- 0.00025  # Ajustar si se conoce valor exacto

# Unir todos los años
sf_2023$Year <- "2023"
sf_2024$Year <- "2024"
sf_2025$Year <- "2025"

sf_all <- rbind(sf_2023, sf_2024, sf_2025)

# Lista de ID únicos
all_ids <- unique(sf_all$ID_Poly_po)

# Histogramas por año e ID
for (year in c("2023", "2024", "2025")) {
  sf_year <- sf_all %>% filter(Year == year)
  unique_ids <- unique(sf_year$ID_Poly_po)
  area_per_pixel <- get(paste0("area_per_pixel_", year))
  
  for (id in unique_ids) {
    subset_data <- sf_year %>% filter(ID_Poly_po == id)
    subset_data <- subset_data[subset_data$Mean_ndvi_ >= -1 & subset_data$Mean_ndvi_ <= 1, ]
    if (nrow(subset_data) > 0) {
      p <- ggplot(subset_data, aes(x = Mean_ndvi_)) +
        geom_histogram(aes(y = ..count.. * area_per_pixel), bins = 10, fill = "blue", color = "black") +
        ggtitle(paste("Histogram of Mean_ndvi_ for ID_Poly_po:", id, "(", year, ")")) +
        xlab("Mean_ndvi_") +
        ylab("Área (m2)") +
        xlim(-1, 1)
      print(p)
    } else {
      print(paste("No valid data for ID_Poly_po:", id, "(", year, ")"))
    }
  }
}

# Calcular percentiles por año
percentiles_2023 <- quantile(sf_2023$Mean_ndvi_, probs = seq(0.1, 1, 0.1), na.rm = TRUE)
percentiles_2024 <- quantile(sf_2024$Mean_ndvi_, probs = seq(0.1, 1, 0.1), na.rm = TRUE)
percentiles_2025 <- quantile(sf_2025$Mean_ndvi_, probs = seq(0.1, 1, 0.1), na.rm = TRUE)

# Crear categorías
sf_2023$NDVI_Category <- cut(sf_2023$Mean_ndvi_, breaks = c(-Inf, percentiles_2023, Inf), labels = 1:11)
sf_2024$NDVI_Category <- cut(sf_2024$Mean_ndvi_, breaks = c(-Inf, percentiles_2024, Inf), labels = 1:11)
sf_2025$NDVI_Category <- cut(sf_2025$Mean_ndvi_, breaks = c(-Inf, percentiles_2025, Inf), labels = 1:11)

# Reunir todo nuevamente
sf_all <- rbind(sf_2023, sf_2024, sf_2025)

# Crear tabla de frecuencias por ID y Año
todos_los_deciles <- data.frame(NDVI_Category = as.factor(1:10))
all_combined_frequencies <- data.frame()

for (id in all_ids) {
  datos <- sf_all %>% filter(ID_Poly_po == id & !is.na(NDVI_Category)) %>% st_drop_geometry()
  freqs <- datos %>% group_by(NDVI_Category, Year) %>% summarise(Freq = n(), .groups = 'drop')
  freqs$ID_Poly_po <- id
  all_combined_frequencies <- rbind(all_combined_frequencies, freqs)
}

# Calcular área
all_combined_frequencies <- all_combined_frequencies %>%
  mutate(Area = case_when(
    Year == "2023" ~ Freq * area_per_pixel_2023,
    Year == "2024" ~ Freq * area_per_pixel_2024,
    Year == "2025" ~ Freq * area_per_pixel_2025
  ))

# Gráficos
for (id in unique(all_combined_frequencies$ID_Poly_po)) {
  df <- all_combined_frequencies %>% filter(ID_Poly_po == id)
  p <- ggplot(df, aes(x = NDVI_Category, y = Area, fill = Year)) +
    geom_bar(stat = "identity", position = position_dodge()) +
    geom_vline(xintercept = 5.5, linetype = "dashed", color = "red", size = 1) +
    annotate("text", x = 5.5, y = max(df$Area, na.rm = TRUE), label = "Límite estimado de la actividad fotosintética", 
             angle = 90, vjust = -0.5, hjust = 1, color = "red") +
    scale_y_continuous(labels = function(x) format(x, big.mark = ".", decimal.mark = ",")) +
    labs(title = paste("Categorías de NDVI para la Unidad n°", id), x = "Categoría de NDVI", y = "Área (m2)", fill = "Año") +
    theme_minimal()
  print(p)
}

# Calcular distancia de Wasserstein por ID (2023 vs 2024 y 2024 vs 2025)
comparaciones <- list(c("2023", "2024"), c("2024", "2025"))

for (id in all_ids) {
  for (comp in comparaciones) {
    a <- sf_all %>% filter(ID_Poly_po == id, Year == comp[1], !is.na(Mean_ndvi_)) %>% pull(Mean_ndvi_)
    b <- sf_all %>% filter(ID_Poly_po == id, Year == comp[2], !is.na(Mean_ndvi_)) %>% pull(Mean_ndvi_)
    
    # Crear histogramas normalizados
    breaks <- seq(-1, 1, length.out = 11)
    hist_a <- hist(a, breaks = breaks, plot = FALSE)$counts
    hist_b <- hist(b, breaks = breaks, plot = FALSE)$counts
    
    if (sum(hist_a) > 0 && sum(hist_b) > 0) {
      norm_a <- hist_a / sum(hist_a)
      norm_b <- hist_b / sum(hist_b)
      cost_matrix <- as.matrix(dist(1:10, method = "manhattan"))
      d_w <- wasserstein(norm_a, norm_b, costm = cost_matrix)
      print(paste("Wasserstein distance para ID_Poly_po", id, "de", comp[1], "a", comp[2], ":", round(d_w, 4)))
    } else {
      print(paste("Datos insuficientes para ID", id, "entre", comp[1], "y", comp[2]))
    }
  }
}
